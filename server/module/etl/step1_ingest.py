"""
ETL Step 1: ë°ì´í„° ìˆ˜ì§‘ (ìµœì í™”)
- ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ + curl_cffi
- ì¬ì‹œë„ + ì§€ìˆ˜ ë°±ì˜¤í”„
- Fallback ë©”ì»¤ë‹ˆì¦˜
"""

import os
import sys
import time
from typing import Dict, List, Optional, Tuple

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + "/../..")

from datetime import datetime, timedelta

import pandas as pd
import pyarrow as pa
import pyarrow.fs as pafs
import pyarrow.parquet as pq
import yfinance as yf
from curl_cffi import requests as curl_requests
from module.etl.config import get_staging_path
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker


def get_stocks_to_update(
    iso_code: str, database_url: str, target_date: datetime.date
) -> pd.DataFrame:
    """
    ì—…ë°ì´íŠ¸ ëŒ€ìƒ ì¢…ëª© ì¡°íšŒ

    Returns:
        DataFrame [meta_id, ticker, name, start_date]
    """
    engine = create_engine(database_url, pool_pre_ping=True)

    lookback_days = 7

    # Parameterized queryë¡œ SQL Injection ë°©ì§€
    sql = text(
        """
        SELECT
            meta_id,
            ticker,
            name,
            CASE
                WHEN max_date IS NULL THEN
                    DATE '2000-01-01'
                ELSE
                    max_date - INTERVAL :lookback_days
            END as start_date
        FROM tb_meta
        WHERE iso_code = :iso_code
        AND (max_date IS NULL OR max_date < :target_date)
        ORDER BY start_date, meta_id
    """
    )

    df = pd.read_sql(
        sql,
        engine,
        params={
            "iso_code": iso_code,
            "target_date": target_date,
            "lookback_days": f"{lookback_days} days",
        },
    )
    engine.dispose()

    # KR ì¢…ëª©: Yahoo FinanceëŠ” .KS/.KQ ì ‘ë¯¸ì‚¬ í•„ìš”
    # (KOSPI: .KS, KOSDAQ: .KQ - í˜„ì¬ëŠ” ëª¨ë‘ .KSë¡œ ì‹œë„, ì¶”í›„ ê°œì„  í•„ìš”)
    if iso_code == "KR":
        df["ticker"] = df["ticker"] + ".KS"

    return df


def download_batch_with_retry(
    tickers: List[str], start_date: datetime.date, end_date: datetime.date, max_retries: int = 3
) -> Tuple[pd.DataFrame, bool]:
    """
    ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ (curl_cffi + ì¬ì‹œë„)

    Returns:
        (DataFrame, success: bool)
    """
    # curl_cffi ì„¸ì…˜ ìƒì„±
    session = curl_requests.Session(impersonate="chrome")

    try:
        for attempt in range(max_retries):
            try:
                print(f"      ì‹œë„ {attempt + 1}/{max_retries}...")

                # yf.download()ì— curl_cffi ì„¸ì…˜ ì ìš©
                df = yf.download(
                    tickers=tickers,
                    start=start_date,
                    end=end_date,
                    group_by="ticker",
                    progress=False,
                    threads=False,
                    auto_adjust=False,
                    session=session,
                )

                if not df.empty:
                    return df, True

            except Exception as e:
                error_msg = str(e).lower()

                # Rate Limit ê°ì§€
                if "429" in error_msg or "rate limit" in error_msg or "too many" in error_msg:
                    wait = (attempt + 1) ** 2 * 10  # ì§€ìˆ˜ ë°±ì˜¤í”„: 10s, 40s, 90s
                    print(f"      âš ï¸  Rate Limit! {wait}ì´ˆ ëŒ€ê¸°...")
                    time.sleep(wait)
                else:
                    wait = (attempt + 1) * 5
                    print(f"      âš ï¸  ì—ëŸ¬: {str(e)[:50]}... {wait}ì´ˆ í›„ ì¬ì‹œë„")
                    time.sleep(wait)

        return pd.DataFrame(), False

    finally:
        # ì„¸ì…˜ì€ í•­ìƒ ë‹«ê¸°
        try:
            session.close()
        except:
            pass


def download_single_ticker_fallback(
    ticker: str, start_date: datetime.date, end_date: datetime.date
) -> Optional[pd.DataFrame]:
    """
    ê°œë³„ ì¢…ëª© Fallback (ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ)
    """
    session = None
    try:
        session = curl_requests.Session(impersonate="chrome")

        # Ticker().history() ë°©ì‹ (ë‹¤ë¥¸ ì—”ë“œí¬ì¸íŠ¸)
        stock = yf.Ticker(ticker, session=session)
        df = stock.history(start=start_date, end=end_date, auto_adjust=False)

        if not df.empty:
            df = df[["Close", "Adj Close", "Volume"]].reset_index()
            df["ticker"] = ticker
            df.columns = ["trade_date", "close", "adj_close", "volume", "ticker"]
            return df
    except Exception as e:
        print(f"         âš ï¸  {ticker} Fallback ì‹¤íŒ¨: {str(e)[:30]}...")
    finally:
        if session:
            try:
                session.close()
            except:
                pass

    return None


def process_batch_optimized(
    batch_df: pd.DataFrame, target_date: datetime.date
) -> Tuple[pa.Table, List[str]]:
    """
    ë°°ì¹˜ ì²˜ë¦¬ (ìµœì í™”)

    Returns:
        (arrow_table, failed_tickers)
    """
    tickers = batch_df["ticker"].tolist()
    start_date = batch_df["start_date"].min()
    failed_tickers = []

    # 1ì°¨: ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì‹œë„
    price_data, success = download_batch_with_retry(
        tickers=tickers,
        start_date=start_date,
        end_date=target_date + timedelta(days=1),
        max_retries=3,
    )

    results = []

    if success and not price_data.empty:
        # ë°°ì¹˜ ì„±ê³µ: ë°ì´í„° íŒŒì‹±
        if len(tickers) == 1:
            ticker = tickers[0]
            df = price_data[["Close", "Adj Close", "Volume"]].copy()
            df = df.reset_index()  # ğŸ”½ ì¶”ê°€ (indexë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ)
            df["ticker"] = ticker
            df.columns = ["trade_date", "close", "adj_close", "volume", "ticker"]
            results.append(df)
        else:
            for ticker in tickers:
                try:
                    ticker_data = price_data[ticker][["Close", "Adj Close", "Volume"]].copy()
                    ticker_data = ticker_data.reset_index()  # ğŸ”½ ì¶”ê°€
                    ticker_data["ticker"] = ticker
                    ticker_data.columns = ["trade_date", "close", "adj_close", "volume", "ticker"]
                    results.append(ticker_data)
                except (KeyError, AttributeError):
                    failed_tickers.append(ticker)
    else:
        # ë°°ì¹˜ ì‹¤íŒ¨: ê°œë³„ Fallback
        print(f"      âš ï¸  ë°°ì¹˜ ì‹¤íŒ¨, ê°œë³„ Fallback ì‹œë„...")

        for ticker in tickers:
            df = download_single_ticker_fallback(
                ticker, start_date, target_date + timedelta(days=1)
            )

            if df is not None and not df.empty:
                results.append(df)
            else:
                failed_tickers.append(ticker)

            # ê°œë³„ ìš”ì²­ ê°„ ë”œë ˆì´
            time.sleep(1)

    if not results:
        return None, failed_tickers

    # DataFrame í†µí•©
    combined = pd.concat(results, ignore_index=True)

    combined["ticker"] = combined["ticker"].astype(str)
    combined["trade_date"] = pd.to_datetime(combined["trade_date"]).dt.date

    # meta_id, name ì¶”ê°€ (tickerë¡œ ì¡°ì¸)
    batch_df_copy = batch_df.copy()
    batch_df_copy["ticker"] = batch_df_copy["ticker"].astype(str)

    combined = combined.merge(batch_df_copy[["ticker", "meta_id", "name"]], on="ticker", how="left")

    # KR ì¢…ëª©: .KS/.KQ ì ‘ë¯¸ì‚¬ ì œê±° (ì›ë˜ tickerë¡œ ë³µì›)
    combined["ticker"] = combined["ticker"].str.replace(".KS", "", regex=False)
    combined["ticker"] = combined["ticker"].str.replace(".KQ", "", regex=False)

    # ì •ì œ
    combined = combined.dropna(subset=["close", "adj_close"])
    combined["volume"] = combined["volume"].fillna(0).astype("int64")
    combined["gross_return"] = None
    combined["updated_at"] = datetime.now()

    # Arrow ë³€í™˜
    arrow_schema = pa.schema(
        [
            pa.field("meta_id", pa.int32(), nullable=False),
            pa.field("trade_date", pa.date32(), nullable=False),
            pa.field("ticker", pa.string(), nullable=False),
            pa.field("name", pa.string(), nullable=True),
            pa.field("close", pa.float64(), nullable=True),
            pa.field("adj_close", pa.float64(), nullable=True),
            pa.field("gross_return", pa.float64(), nullable=True),
            pa.field("volume", pa.int64(), nullable=True),
            pa.field("updated_at", pa.timestamp("us"), nullable=False),
        ]
    )

    arrow_table = pa.Table.from_pandas(
        combined[
            [
                "meta_id",
                "trade_date",
                "ticker",
                "name",
                "close",
                "adj_close",
                "gross_return",
                "volume",
                "updated_at",
            ]
        ],
        schema=arrow_schema,
    )

    return arrow_table, failed_tickers


def ingest_daily_data(
    iso_code: str, date: datetime.date, database_url: str
) -> Tuple[int, List[str]]:
    """
    ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ (ìµœì í™”)

    Returns:
        (success_count, failed_tickers)
    """
    print(f"\n{'='*70}")
    print(f"ğŸ“¥ [{iso_code}] {date} ë°ì´í„° ìˆ˜ì§‘ (ìµœì í™”)")
    print(f"{'='*70}")

    # 1. ëŒ€ìƒ ì¢…ëª©
    print("\n1ï¸âƒ£ ëŒ€ìƒ ì¢…ëª© ì¡°íšŒ ì¤‘...")
    stocks_df = get_stocks_to_update(iso_code, database_url, date)

    print(f"   âœ… {len(stocks_df)} ì¢…ëª©")

    if len(stocks_df) == 0:
        return 0, []

    # 2. ë°°ì¹˜ ì²˜ë¦¬
    print(f"\n2ï¸âƒ£ Yahoo Finance ë°°ì¹˜ ë‹¤ìš´ë¡œë“œ ì¤‘...")

    batch_size = 150  # ë³´ìˆ˜ì ì¸ ë°°ì¹˜ í¬ê¸°
    arrow_tables = []
    all_failed = []
    total_success = 0

    for batch_idx in range(0, len(stocks_df), batch_size):
        batch = stocks_df.iloc[batch_idx : batch_idx + batch_size]
        batch_num = batch_idx // batch_size + 1
        total_batches = (len(stocks_df) + batch_size - 1) // batch_size

        print(f"\n   ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ({len(batch)} ì¢…ëª©)")

        arrow_table, failed = process_batch_optimized(batch, date)

        if arrow_table is not None:
            arrow_tables.append(arrow_table)
            success = len(batch) - len(failed)
            total_success += success
            print(f"      âœ… ì„±ê³µ: {success}/{len(batch)}")

            if failed:
                print(f"      âš ï¸  ì‹¤íŒ¨: {len(failed)}ê°œ")
                all_failed.extend(failed)
        else:
            print(f"      âŒ ì „ì²´ ì‹¤íŒ¨")
            all_failed.extend(batch["ticker"].tolist())

        # ë°°ì¹˜ ê°„ ì¶©ë¶„í•œ ë”œë ˆì´ (Rate Limit ë°©ì§€)
        if batch_idx + batch_size < len(stocks_df):
            wait_time = 8  # 8ì´ˆ ëŒ€ê¸°
            print(f"      â³ {wait_time}ì´ˆ ëŒ€ê¸° ì¤‘...")
            time.sleep(wait_time)

    print(f"\n   ğŸ“Š ìµœì¢…: {total_success}/{len(stocks_df)} ì„±ê³µ, {len(all_failed)} ì‹¤íŒ¨")

    if not arrow_tables:
        print("   âš ï¸  ìˆ˜ì§‘ ë°ì´í„° ì—†ìŒ")
        return 0, all_failed

    # 3. í†µí•© ë° ì €ì¥
    print(f"\n3ï¸âƒ£ S3 ì €ì¥ ì¤‘...")
    combined_table = pa.concat_tables(arrow_tables)

    staging_path = get_staging_path(iso_code, date.strftime("%Y-%m-%d"))
    s3_fs = pafs.S3FileSystem(region="ap-northeast-2")

    with s3_fs.open_output_stream(staging_path.replace("s3://", "")) as f:
        pq.write_table(combined_table, f, compression="snappy")

    print(f"   âœ… {staging_path}")
    print(f"   ğŸ“Š {len(combined_table)} rows, {combined_table.nbytes / 1024 / 1024:.2f} MB")

    return total_success, all_failed


if __name__ == "__main__":
    print("=" * 70)
    print("ğŸš€ Step 1: Ingest (ìµœì í™”)")
    print("=" * 70)

    database_url = os.getenv("DATABASE_URL")
    if not database_url:
        raise ValueError("DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤!")

    target_date = datetime.now().date() - timedelta(days=1)

    print(f"\nğŸ“… ë‚ ì§œ: {target_date}")
    print(f"ğŸŒ êµ­ê°€: US")
    print(f"â±ï¸  ì˜ˆìƒ: 20~30ë¶„")
    print(f"ğŸ“‹ ì „ëµ: ë°°ì¹˜(150) + curl_cffi + ì¬ì‹œë„ + Fallback\n")

    try:
        success, failed = ingest_daily_data("US", target_date, database_url)

        print("\n" + "=" * 70)
        print(f"âœ… ì™„ë£Œ: {success} ì¢…ëª©")

        if failed:
            print(f"âš ï¸  ì‹¤íŒ¨: {len(failed)} ì¢…ëª©")
            # ì‹¤íŒ¨ ëª©ë¡ ì €ì¥ (ë‚˜ì¤‘ì— ì¬ì‹œë„ìš©)
            failed_file = f"/tmp/failed_tickers_{target_date}.txt"
            with open(failed_file, "w") as f:
                f.write("\n".join(failed))
            print(f"   ì‹¤íŒ¨ ëª©ë¡: {failed_file}")

        print("=" * 70)

        staging_path = get_staging_path("US", target_date.strftime("%Y-%m-%d"))
        print(f"\nğŸ’¾ {staging_path}")

    except Exception as e:
        print(f"\nâŒ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        exit(1)
