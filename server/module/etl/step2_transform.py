"""
ETL Step 2: ë°ì´í„° ë³€í™˜ (Transform)
- Staging ë°ì´í„° ì½ê¸°
- Iceberg ê¸°ì¤€ìœ¼ë¡œ adj_close ì¬ê³„ì‚°
- gross_return ê³„ì‚°
"""

import os
import sys

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + "/../..")

from datetime import date, datetime, timedelta
from typing import Dict, Tuple

import pandas as pd
import pyarrow as pa
import pyarrow.compute as pc
import pyarrow.parquet as pq
from module.etl.config import (
    TABLE_KR_STOCKS,
    TABLE_US_STOCKS,
    get_latest_staging_file,
    get_transformed_path,
)


def get_last_iceberg_data(iso_code: str, meta_ids: list) -> Dict[int, dict]:
    """
    Athenaë¡œ meta_idë³„ ìµœì‹  ë°ì´í„° ì¡°íšŒ (ë‚ ì§œ ì œí•œ ì—†ìŒ)

    Args:
        iso_code: "US" ë˜ëŠ” "KR"
        meta_ids: ì¢…ëª© ID ë¦¬ìŠ¤íŠ¸

    Returns:
        {meta_id: {'trade_date': date, 'adj_close': float}}
    """
    import time
    from datetime import datetime as dt

    import boto3

    # ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²´í¬
    if not meta_ids:
        print(f"   âš ï¸  ì¡°íšŒí•  meta_idê°€ ì—†ìŠµë‹ˆë‹¤")
        return {}

    athena = boto3.client("athena", region_name="ap-northeast-2")

    table_name = "us_stocks_price" if iso_code == "US" else "kr_stocks_price"
    meta_id_list = ",".join(map(str, meta_ids))

    # Athena ì¿¼ë¦¬: meta_idë³„ ìµœì‹  ë°ì´í„°
    query = f"""
        SELECT
            meta_id,
            MAX(trade_date) as last_date,
            MAX_BY(adj_close, trade_date) as adj_close
        FROM market.{table_name}
        WHERE meta_id IN ({meta_id_list})
        GROUP BY meta_id
    """

    print(f"   ğŸ” Athena ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘... ({len(meta_ids)} ì¢…ëª©)")

    # ì¿¼ë¦¬ ì‹¤í–‰ (ê²°ê³¼ëŠ” datalake ë²„í‚·ì— ì €ì¥)
    try:
        response = athena.start_query_execution(
            QueryString=query,
            QueryExecutionContext={"Database": "market"},
            ResultConfiguration={"OutputLocation": "s3://insight-invest-datalake/athena-results/"},
        )
    except Exception as e:
        print(f"   âŒ Athena ì¿¼ë¦¬ ì‹œì‘ ì‹¤íŒ¨: {e}")
        return {}

    query_id = response["QueryExecutionId"]

    # ëŒ€ê¸° (ìµœëŒ€ 300ì´ˆë¡œ ì¦ê°€ - ëŒ€ëŸ‰ ë°ì´í„° ëŒ€ì‘)
    query_succeeded = False
    max_wait_time = 300  # 5ë¶„
    for i in range(max_wait_time):
        status_response = athena.get_query_execution(QueryExecutionId=query_id)
        status = status_response["QueryExecution"]["Status"]["State"]

        if status == "SUCCEEDED":
            query_succeeded = True
            break
        elif status in ["FAILED", "CANCELLED"]:
            reason = status_response["QueryExecution"]["Status"].get("StateChangeReason", "")
            print(f"   âŒ Athena ì¿¼ë¦¬ ì‹¤íŒ¨: {reason}")
            return {}

        if i % 10 == 0 and i > 0:
            print(f"      ëŒ€ê¸° ì¤‘... ({i}ì´ˆ)")
        time.sleep(1)

    # íƒ€ì„ì•„ì›ƒ ì²´í¬
    if not query_succeeded:
        print(f"   âŒ Athena ì¿¼ë¦¬ íƒ€ì„ì•„ì›ƒ ({max_wait_time}ì´ˆ)")
        return {}

    # ê²°ê³¼ ì¡°íšŒ (í˜ì´ì§• ì²˜ë¦¬)
    last_data = {}
    next_token = None

    while True:
        if next_token:
            result_response = athena.get_query_results(
                QueryExecutionId=query_id, NextToken=next_token
            )
        else:
            result_response = athena.get_query_results(QueryExecutionId=query_id)

        # ì²« ë²ˆì§¸ í˜¸ì¶œì—ì„œë§Œ í—¤ë” ê±´ë„ˆëœ€
        rows = result_response["ResultSet"]["Rows"]
        start_idx = 1 if next_token is None else 0

        for row in rows[start_idx:]:
            try:
                meta_id = int(row["Data"][0]["VarCharValue"])
                last_date_str = row["Data"][1]["VarCharValue"]
                adj_close = float(row["Data"][2]["VarCharValue"])

                # ë‚ ì§œ íŒŒì‹±
                last_date = dt.strptime(last_date_str, "%Y-%m-%d").date()

                last_data[meta_id] = {"trade_date": last_date, "adj_close": adj_close}
            except (KeyError, ValueError, IndexError):
                continue

        # ë‹¤ìŒ í˜ì´ì§€ í™•ì¸
        next_token = result_response.get("NextToken")
        if not next_token:
            break

    print(f"   âœ… Athena: {len(last_data)}/{len(meta_ids)} ì¢…ëª©")

    return last_data


def recalculate_adj_close_and_returns(
    arrow_table: pa.Table, last_iceberg_data: Dict[int, dict]
) -> pa.Table:
    """
    Iceberg ê¸°ì¤€ìœ¼ë¡œ adj_close ì¬ê³„ì‚° + ì¤‘ë³µ ë°ì´í„° ì œê±°

    ë¡œì§:
    1. Iceberg ë§ˆì§€ë§‰ ë‚ ì§œì˜ adj_close = ê¸°ì¤€ì 
    2. Yahoo ì†Œì‹± ë°ì´í„°ì—ì„œ ê°™ì€ ë‚ ì§œì˜ adj_close = Yahoo ê¸°ì¤€
    3. adj_close_new = adj_close_iceberg * (adj_close_yf / adj_close_yf_base)
    4. gross_return = (adj_close_new - adj_close_prev) / adj_close_prev
    5. Iceberg ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ ë°ì´í„°ë§Œ ìœ ì§€ (ì¤‘ë³µ ë°©ì§€)

    Args:
        arrow_table: Staging ë°ì´í„° (Yahoo Finance)
        last_iceberg_data: Icebergì˜ ë§ˆì§€ë§‰ ë°ì´í„°

    Returns:
        ì¬ê³„ì‚°ëœ Arrow Table (Iceberg ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ë§Œ)
    """
    from datetime import date as date_type

    try:
        # Pandas ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)
        needed_cols = [
            "meta_id",
            "trade_date",
            "ticker",
            "name",
            "close",
            "adj_close",
            "volume",
            "updated_at",
        ]
        df = arrow_table.select(needed_cols).to_pandas()

        # trade_dateë¥¼ datetime.dateë¡œ í†µì¼ (íƒ€ì… ë¶ˆì¼ì¹˜ ë°©ì§€)
        if pd.api.types.is_datetime64_any_dtype(df["trade_date"]):
            df["trade_date"] = df["trade_date"].dt.date
        elif not df["trade_date"].apply(lambda x: isinstance(x, date_type)).all():
            df["trade_date"] = pd.to_datetime(df["trade_date"]).dt.date

        df = df.sort_values(["meta_id", "trade_date"]).reset_index(drop=True)

        df["adj_close_recalc"] = None
        df["gross_return"] = None
        df["_keep"] = True  # ìœ ì§€í•  í–‰ í‘œì‹œ

        # ì¢…ëª©ë³„ ì²˜ë¦¬
        for meta_id in df["meta_id"].unique():
            mask = df["meta_id"] == meta_id
            meta_df = df[mask].copy()

            # Iceberg ê¸°ì¤€ì 
            if meta_id in last_iceberg_data:
                last_date = last_iceberg_data[meta_id]["trade_date"]
                adj_close_iceberg = last_iceberg_data[meta_id]["adj_close"]

                # Yahoo ë°ì´í„°ì—ì„œ ê°™ì€ ë‚ ì§œ ì°¾ê¸° (íƒ€ì… í†µì¼ë¨)
                same_date_row = meta_df[meta_df["trade_date"] == last_date]

                if not same_date_row.empty:
                    # Yahooì˜ ê°™ì€ ë‚ ì§œ adj_close
                    adj_close_yf_base = same_date_row.iloc[0]["adj_close"]
                else:
                    # ê°™ì€ ë‚ ì§œ ì—†ìœ¼ë©´ Yahoo ì²« ê°’ ì‚¬ìš©
                    adj_close_yf_base = meta_df.iloc[0]["adj_close"]

                # ì¤‘ë³µ ì œê±°: Iceberg ë§ˆì§€ë§‰ ë‚ ì§œ ì´ì „/ë™ì¼ ë°ì´í„°ëŠ” ì œì™¸
                df.loc[mask & (df["trade_date"] <= last_date), "_keep"] = False
            else:
                # Icebergì— ë°ì´í„° ì—†ìŒ â†’ Yahoo ì²« ê°’ì´ ê¸°ì¤€ (ì‹ ê·œ ì¢…ëª©)
                adj_close_iceberg = meta_df.iloc[0]["adj_close"]
                adj_close_yf_base = meta_df.iloc[0]["adj_close"]
                last_date = None  # ì‹ ê·œ ì¢…ëª©ì€ ëª¨ë“  ë°ì´í„° ìœ ì§€

            # ê³„ì‚°
            adj_closes = []
            gross_returns = []

            # Icebergì— ë°ì´í„° ìˆìœ¼ë©´ prev = Iceberg ë§ˆì§€ë§‰, ì—†ìœ¼ë©´ None (ì‹ ê·œ ì¢…ëª© ì²« í–‰ì€ None)
            prev_adj_close = adj_close_iceberg if meta_id in last_iceberg_data else None

            for idx, row in meta_df.iterrows():
                adj_close_yf = row["adj_close"]

                # Yahoo ë³€í™”ìœ¨ì„ Iceberg ê¸°ì¤€ì— ì ìš©
                if adj_close_yf_base and adj_close_yf_base > 0:
                    adj_close_recalc = adj_close_iceberg * (adj_close_yf / adj_close_yf_base)
                else:
                    adj_close_recalc = adj_close_yf

                # gross_return ê³„ì‚°
                if prev_adj_close and prev_adj_close > 0:
                    gross_return = (adj_close_recalc - prev_adj_close) / prev_adj_close
                else:
                    gross_return = None

                adj_closes.append(adj_close_recalc)
                gross_returns.append(gross_return)

                # ë‹¤ìŒì„ ìœ„í•œ ì—…ë°ì´íŠ¸
                prev_adj_close = adj_close_recalc

            # DataFrameì— ë°˜ì˜
            df.loc[mask, "adj_close_recalc"] = adj_closes
            df.loc[mask, "gross_return"] = gross_returns

        # adj_close êµì²´
        df["adj_close"] = df["adj_close_recalc"]
        df = df.drop(columns=["adj_close_recalc"])

        # ì¤‘ë³µ ë°ì´í„° ì œê±°: Iceberg ë§ˆì§€ë§‰ ë‚ ì§œ ì´í›„ë§Œ ìœ ì§€
        df = df[df["_keep"]].drop(columns=["_keep"])

        if df.empty:
            return None

        # Arrow Tableë¡œ ë³€í™˜
        return pa.Table.from_pandas(df[arrow_table.column_names], schema=arrow_table.schema)

    except Exception as e:
        print(f"      âŒ ì¬ê³„ì‚° ì¤‘ ì—ëŸ¬: {e}")
        import traceback

        traceback.print_exc()
        return None


def validate_data(arrow_table: pa.Table) -> Tuple[pa.Table, list]:
    """
    ë°ì´í„° ê²€ì¦ ë° ì´ìƒì¹˜ ì œê±°

    ê²€ì¦ í•­ëª©:
        1. ê²°ì¸¡ì¹˜ ì²´í¬ (close, adj_close, volume)
        2. ê°€ê²© ì´ìƒì¹˜ (adj_close <= 0)
        3. ìˆ˜ìµë¥  ì´ìƒì¹˜ (gross_return > Â±50%)
        4. ê±°ë˜ëŸ‰ 0 ì²´í¬
        5. ê±°ë˜ì¼ ì—°ì†ì„± ì²´í¬ (5ì¼ ì´ìƒ ê°­)

    Returns:
        (cleaned_table, warnings)
    """
    warnings = []
    total_rows = len(arrow_table)

    # 1. ê²°ì¸¡ì¹˜ ì²´í¬
    null_counts = {
        col: pc.sum(pc.is_null(arrow_table[col])).as_py()
        for col in ["close", "adj_close", "volume"]
    }

    for col, count in null_counts.items():
        if count > 0:
            warnings.append(f"âš ï¸ {col}: {count} null values ({count/total_rows*100:.1f}%)")

    # 2. ì´ìƒì¹˜ ì²´í¬ (adj_close <= 0)
    invalid_prices = pc.sum(pc.less_equal(arrow_table["adj_close"], 0)).as_py()

    if invalid_prices > 0:
        warnings.append(f"âŒ Invalid prices (<=0): {invalid_prices}")

    # 3. gross_return ì´ìƒì¹˜ ì²´í¬ (Â±50% ì´ˆê³¼)
    if "gross_return" in arrow_table.column_names:
        gross_returns = arrow_table.column("gross_return")

        # Â±50% ì´ˆê³¼ ì²´í¬
        extreme_positive = pc.sum(pc.greater(gross_returns, 0.5)).as_py()
        extreme_negative = pc.sum(pc.less(gross_returns, -0.5)).as_py()

        if extreme_positive > 0:
            warnings.append(f"âš ï¸ Extreme positive returns (>50%): {extreme_positive}")

        if extreme_negative > 0:
            warnings.append(f"âš ï¸ Extreme negative returns (<-50%): {extreme_negative}")

    # 4. volume 0 ì²´í¬
    zero_volume = pc.sum(pc.equal(arrow_table["volume"], 0)).as_py()

    if zero_volume > 0:
        pct = zero_volume / total_rows * 100
        # 10% ì´ìƒì´ë©´ ê²½ê³ , ì•„ë‹ˆë©´ ì •ë³´
        if pct > 10:
            warnings.append(f"âš ï¸ Zero volume: {zero_volume} ({pct:.1f}%)")
        else:
            warnings.append(f"â„¹ï¸ Zero volume: {zero_volume} ({pct:.1f}%)")

    # 5. ê±°ë˜ì¼ ì—°ì†ì„± ì²´í¬ (ì¢…ëª©ë³„ 5ì¼ ì´ìƒ ê°­)
    df = arrow_table.to_pandas()
    df["trade_date"] = pd.to_datetime(df["trade_date"])

    gap_warnings = []
    for meta_id, group in df.groupby("meta_id"):
        if len(group) < 2:
            continue

        sorted_dates = group["trade_date"].sort_values()
        gaps = sorted_dates.diff().dropna()

        # 5 ì˜ì—…ì¼ ì´ìƒ ê°­ (ì•½ 7ì¼)
        large_gaps = gaps[gaps > pd.Timedelta(days=7)]
        if len(large_gaps) > 0:
            ticker = group["ticker"].iloc[0]
            gap_warnings.append(f"{ticker}: {len(large_gaps)} large gaps")

    if gap_warnings:
        warnings.append(f"â„¹ï¸ Trading day gaps (>7 days): {len(gap_warnings)} tickers")

    # 6. ë°ì´í„° ì •ì œ: adj_close > 0ì¸ ê²ƒë§Œ
    mask = pc.greater(arrow_table["adj_close"], 0)
    cleaned_table = arrow_table.filter(mask)

    removed_count = total_rows - len(cleaned_table)
    if removed_count > 0:
        warnings.append(f"ğŸ—‘ï¸ Removed {removed_count} invalid rows")

    return cleaned_table, warnings


def transform_daily_data(iso_code: str, date: date) -> Tuple[pa.Table, int]:
    """
    ì¼ë³„ ë°ì´í„° ë³€í™˜ (Iceberg ê¸°ì¤€ ì¬ê³„ì‚°)

    Args:
        iso_code: "US" ë˜ëŠ” "KR"
        date: ëŒ€ìƒ ë‚ ì§œ

    Returns:
        (transformed_table, row_count)
    """
    import gc

    print(f"\n{'='*70}")
    print(f"ğŸ”„ [{iso_code}] {date} ë°ì´í„° ë³€í™˜ (Iceberg ê¸°ì¤€)")
    print(f"{'='*70}")

    # 1. Staging ë°ì´í„° ì½ê¸°
    print("\n1ï¸âƒ£ Staging ë°ì´í„° ì½ê¸° ì¤‘...")

    try:
        staging_path = get_latest_staging_file(iso_code, date.strftime("%Y-%m-%d"))
        arrow_table = pq.read_table(staging_path)
        print(f"   âœ… {len(arrow_table)} rows")
        print(f"   ğŸ“‚ {staging_path}")
    except FileNotFoundError as e:
        print(f"   âŒ Staging íŒŒì¼ ì—†ìŒ: {e}")
        return None, 0

    # 2. Staging ë°ì´í„° ë‚ ì§œ ë²”ìœ„
    trade_dates = arrow_table.column("trade_date").to_pylist()
    min_date = min(trade_dates)
    max_date = max(trade_dates)

    print(f"   ğŸ“… Staging ë²”ìœ„: {min_date} ~ {max_date}")

    # 3. ì „ì²´ meta_idì— ëŒ€í•´ Iceberg ê¸°ì¤€ì  1íšŒ ì¡°íšŒ (ìµœì í™”)
    print(f"\n2ï¸âƒ£ Iceberg ê¸°ì¤€ì  ì¡°íšŒ (1íšŒ)...")

    all_meta_ids = list(set(arrow_table.column("meta_id").to_pylist()))
    print(f"   ğŸ“Š ì´ {len(all_meta_ids)} ì¢…ëª©")

    # ì „ì²´ meta_idì— ëŒ€í•´ í•œ ë²ˆì— Athena ì¿¼ë¦¬ (ìºì‹±)
    last_iceberg_data = get_last_iceberg_data(iso_code, all_meta_ids)
    print(f"   âœ… Iceberg ê¸°ì¤€ì : {len(last_iceberg_data)}/{len(all_meta_ids)} ì¢…ëª©")

    # 4. ë°°ì¹˜ë³„ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨í™”, Athena í˜¸ì¶œ ì—†ì´ ìºì‹œ ì‚¬ìš©)
    print(f"\n3ï¸âƒ£ ë°°ì¹˜ë³„ ì²˜ë¦¬ ì‹œì‘ (ìºì‹œ ì‚¬ìš©)...")

    batch_size = 500  # ë°°ì¹˜ í¬ê¸°: 500ê°œ ì¢…ëª©ì”©
    processed_tables = []

    for i in range(0, len(all_meta_ids), batch_size):
        batch_meta_ids = all_meta_ids[i : i + batch_size]
        batch_num = i // batch_size + 1
        total_batches = (len(all_meta_ids) + batch_size - 1) // batch_size

        print(f"\n   ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ({len(batch_meta_ids)} ì¢…ëª©)")

        try:
            # ë°°ì¹˜ í•„í„°ë§
            batch_filter = pc.is_in(arrow_table["meta_id"], pa.array(batch_meta_ids))
            batch_table = arrow_table.filter(batch_filter)

            print(f"      ë°ì´í„°: {len(batch_table)} rows")

            # ìºì‹œëœ Iceberg ê¸°ì¤€ì  ì‚¬ìš© (Athena í˜¸ì¶œ ì—†ìŒ)
            batch_iceberg_data = {
                mid: last_iceberg_data[mid] for mid in batch_meta_ids if mid in last_iceberg_data
            }
            print(f"      Iceberg: {len(batch_iceberg_data)}/{len(batch_meta_ids)} ì¢…ëª© (ìºì‹œ)")

            # ì¬ê³„ì‚° + ì¤‘ë³µ ì œê±°
            batch_table = recalculate_adj_close_and_returns(batch_table, batch_iceberg_data)

            if batch_table is not None and len(batch_table) > 0:
                processed_tables.append(batch_table)
                print(f"      âœ… {len(batch_table)} rows ì²˜ë¦¬ ì™„ë£Œ (ì‹ ê·œ ë°ì´í„°)")
            else:
                print(f"      â­ï¸  ì‹ ê·œ ë°ì´í„° ì—†ìŒ (ëª¨ë‘ ê¸°ì¡´ ë°ì´í„°)")

        except Exception as e:
            print(f"      âŒ ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()
            # ë°°ì¹˜ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
            continue

        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()

    # 5. ëª¨ë“  ë°°ì¹˜ í•©ì¹˜ê¸°
    print(f"\n4ï¸âƒ£ ë°°ì¹˜ ë³‘í•© ì¤‘...")

    if not processed_tables:
        print(f"   âš ï¸  ì‹ ê·œ ë°ì´í„° ì—†ìŒ")
        return None, 0

    transformed_table = pa.concat_tables(processed_tables)
    print(f"   âœ… {len(transformed_table)} rows ë³‘í•© ì™„ë£Œ")

    # í†µê³„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ)
    gross_return_col = transformed_table.column("gross_return")
    non_null_count = pc.sum(pc.is_valid(gross_return_col)).as_py()

    if non_null_count > 0:
        print(f"   âœ… {non_null_count}/{len(transformed_table)} rows ê³„ì‚°ë¨")

        # min/max/mean (null ì œì™¸)
        filtered = gross_return_col.filter(pc.is_valid(gross_return_col))
        mean_val = pc.mean(filtered).as_py()
        min_val = pc.min(filtered).as_py()
        max_val = pc.max(filtered).as_py()

        print(f"   ğŸ“Š í‰ê· : {mean_val:.4f}")
        print(f"   ğŸ“Š ë²”ìœ„: [{min_val:.4f}, {max_val:.4f}]")
    else:
        print(f"   âš ï¸  ëª¨ë‘ ìµœì´ˆ ë°ì´í„° (ì´ì „ ê°€ê²© ì—†ìŒ)")

    # 6. ë°ì´í„° ê²€ì¦
    print(f"\n5ï¸âƒ£ ë°ì´í„° ê²€ì¦ ì¤‘...")

    cleaned_table, warnings = validate_data(transformed_table)

    if warnings:
        print(f"   âš ï¸  ê²½ê³ :")
        for warning in warnings[:10]:  # ìµœëŒ€ 10ê°œë§Œ ì¶œë ¥
            print(f"      - {warning}")
        if len(warnings) > 10:
            print(f"      ... ì™¸ {len(warnings) - 10}ê°œ")

    removed = len(transformed_table) - len(cleaned_table)
    if removed > 0:
        print(f"   ğŸ—‘ï¸  ì œê±°: {removed} rows (ì´ìƒì¹˜)")

    print(f"   âœ… ìµœì¢…: {len(cleaned_table)} rows")

    # 7. Transformed ë°ì´í„° S3 ì €ì¥ (ì‹¤íŒ¨ ë³µêµ¬ìš©)
    print(f"\n6ï¸âƒ£ Transformed ë°ì´í„° ì €ì¥ ì¤‘...")

    import pyarrow.fs as pafs

    transformed_path = get_transformed_path(iso_code, date.strftime("%Y-%m-%d"))
    s3_fs = pafs.S3FileSystem(region="ap-northeast-2")

    with s3_fs.open_output_stream(transformed_path.replace("s3://", "")) as f:
        pq.write_table(cleaned_table, f, compression="snappy")

    print(f"   âœ… {transformed_path}")
    print(f"   ğŸ“Š {len(cleaned_table)} rows, {cleaned_table.nbytes / 1024 / 1024:.2f} MB")

    return cleaned_table, len(cleaned_table)


if __name__ == "__main__":
    """
    í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    """
    print("=" * 70)
    print("ğŸš€ Step 2: Transform (Iceberg ê¸°ì¤€)")
    print("=" * 70)

    target_date = datetime.now().date() - timedelta(days=1)

    print(f"\nğŸ“… ë‚ ì§œ: {target_date}")
    print(f"ğŸŒ êµ­ê°€: US\n")

    try:
        transformed_table, row_count = transform_daily_data("US", target_date)

        if transformed_table is not None:
            print("\n" + "=" * 70)
            print(f"âœ… ë³€í™˜ ì™„ë£Œ: {row_count} rows")
            print("=" * 70)

            # ìƒ˜í”Œ ì¶œë ¥
            sample = transformed_table.slice(0, min(5, len(transformed_table)))
            print(f"\nğŸ“Š ë°ì´í„° ìƒ˜í”Œ:")
            print(
                sample.to_pandas()[
                    ["meta_id", "ticker", "trade_date", "close", "adj_close", "gross_return"]
                ]
            )
        else:
            print("\nâŒ ë³€í™˜ ì‹¤íŒ¨")

    except Exception as e:
        print(f"\nâŒ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        exit(1)
