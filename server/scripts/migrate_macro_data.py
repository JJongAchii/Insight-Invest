"""
ë§¤í¬ë¡œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ (PostgreSQL â†’ Iceberg)
"""

import os
import sys
from datetime import date, datetime

from dateutil.relativedelta import relativedelta

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import pyarrow as pa
from pyiceberg.catalog import load_catalog
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

print("=" * 70)
print("ğŸš€ ë§¤í¬ë¡œ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ (Arrow ë„¤ì´í‹°ë¸Œ)")
print("=" * 70)

# RDS ì—°ê²°
database_url = os.getenv("DATABASE_URL")
if not database_url:
    raise ValueError("DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤!")

rds_engine = create_engine(database_url, connect_args={"sslmode": "require"}, pool_pre_ping=True)
SessionLocal = sessionmaker(bind=rds_engine)

# Iceberg ì—°ê²°
catalog = load_catalog(
    "glue",
    **{
        "type": "glue",
        "s3.region": "ap-northeast-2",
        "warehouse": "s3://insight-invest-datalake/warehouse",
    },
)
table = catalog.load_table("market.macro_data")

# ë§ˆì´ê·¸ë ˆì´ì…˜ ê¸°ê°„ ì„¤ì •
START_DATE = date(1800, 1, 1)
END_DATE = date(2025, 12, 1)

# Arrow ìŠ¤í‚¤ë§ˆ ì •ì˜
arrow_schema = pa.schema(
    [
        pa.field("macro_id", pa.int32(), nullable=False),
        pa.field("base_date", pa.date32(), nullable=False),
        pa.field("value", pa.float64(), nullable=True),
        pa.field("fred_series_id", pa.string(), nullable=True),
    ]
)

print(f"\nğŸ“… ë§ˆì´ê·¸ë ˆì´ì…˜ ê¸°ê°„: {START_DATE} ~ {END_DATE}")
print(f"ğŸ’¾ ëŒ€ìƒ í…Œì´ë¸”: market.macro_data")
print("\n" + "=" * 70)

# ì›”ë³„ ë°°ì¹˜ ì²˜ë¦¬
current_date = START_DATE
batch_num = 0
total_rows = 0
failed_batches = []

while current_date < END_DATE:
    batch_num += 1
    month_start = current_date
    month_end = current_date + relativedelta(months=1) - relativedelta(days=1)

    try:
        print(f"\n[ë°°ì¹˜ {batch_num}] {month_start.strftime('%Y-%m')} ì²˜ë¦¬ ì¤‘...")

        # PostgreSQLì—ì„œ ë°ì´í„° ì¡°íšŒ
        with SessionLocal() as session:
            sql = f"""
            SELECT
                d.macro_id,
                d.base_date,
                d.value,
                m.fred as fred_series_id
            FROM tb_macro_data d
            JOIN tb_macro m ON d.macro_id = m.macro_id
            WHERE d.base_date >= '{month_start}'
              AND d.base_date <= '{month_end}'
            ORDER BY d.base_date, d.macro_id
            """

            result = session.execute(text(sql))
            rows = result.fetchall()

        row_count = len(rows)

        if row_count == 0:
            print(f"   âš ï¸  ë°ì´í„° ì—†ìŒ (ìŠ¤í‚µ)")
        else:
            # Arrow Tableë¡œ ë³€í™˜
            data = {
                "macro_id": [row[0] for row in rows],
                "base_date": [row[1] for row in rows],
                "value": [row[2] for row in rows],
                "fred_series_id": [row[3] for row in rows],
            }

            arrow_table = pa.table(data, schema=arrow_schema)

            # Icebergì— ì“°ê¸°
            table.append(arrow_table)

            total_rows += row_count
            print(f"   âœ… {row_count:,} rows ì™„ë£Œ (ëˆ„ì : {total_rows:,})")

    except Exception as e:
        print(f"   âŒ ì—ëŸ¬ ë°œìƒ: {str(e)}")
        failed_batches.append({"month": month_start.strftime("%Y-%m"), "error": str(e)})

    # ë‹¤ìŒ ì›”ë¡œ ì´ë™
    current_date = current_date + relativedelta(months=1)

# ìµœì¢… ê²°ê³¼
print("\n" + "=" * 70)
print("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
print("=" * 70)
print(f"\nì´ ë§ˆì´ê·¸ë ˆì´ì…˜ rows: {total_rows:,}")
print(f"ì²˜ë¦¬í•œ ë°°ì¹˜ ìˆ˜: {batch_num}")
print(f"ì‹¤íŒ¨í•œ ë°°ì¹˜ ìˆ˜: {len(failed_batches)}")

if failed_batches:
    print("\nâš ï¸ ì‹¤íŒ¨í•œ ë°°ì¹˜:")
    for batch in failed_batches:
        print(f"  - {batch['month']}: {batch['error']}")
else:
    print("\nâœ… ëª¨ë“  ë°°ì¹˜ ì„±ê³µ!")

print("\nğŸ’¡ Pandas ì‚¬ìš© ì•ˆ í•¨! 100% Arrow ë„¤ì´í‹°ë¸Œ!")
