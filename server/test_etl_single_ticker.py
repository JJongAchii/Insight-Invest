#!/usr/bin/env python3
"""
ë‹¨ì¼ í‹°ì»¤ ETL í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
SPY ë“± íŠ¹ì • í‹°ì»¤ì— ëŒ€í•´ ì „ì²´ ETL íŒŒì´í”„ë¼ì¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

Usage:
    python test_etl_single_ticker.py --ticker SPY
    python test_etl_single_ticker.py --ticker SPY --from step2
    python test_etl_single_ticker.py --ticker SPY --date 2025-12-01
"""
import argparse
import os
import sys
from datetime import datetime, timedelta

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

import pandas as pd
import pyarrow as pa
import pyarrow.fs as pafs
import pyarrow.parquet as pq
import yfinance as yf
from curl_cffi import requests as curl_requests
from module.etl.config import (
    S3_BUCKET,
    get_latest_staging_file,
    get_latest_transformed_file,
    get_staging_path,
    get_transformed_path,
)
from module.etl.step2_transform import (
    get_last_iceberg_data,
    recalculate_adj_close_and_returns,
    validate_data,
)
from module.etl.step3_load import load_to_iceberg
from sqlalchemy import create_engine, text


def get_ticker_meta(ticker: str, database_url: str) -> dict:
    """í‹°ì»¤ì˜ ë©”íƒ€ ì •ë³´ ì¡°íšŒ"""
    engine = create_engine(database_url, pool_pre_ping=True)

    sql = text(
        """
        SELECT meta_id, ticker, name, iso_code, max_date
        FROM tb_meta
        WHERE ticker = :ticker
    """
    )

    df = pd.read_sql(sql, engine, params={"ticker": ticker})
    engine.dispose()

    if df.empty:
        raise ValueError(f"Ticker '{ticker}' not found in tb_meta")

    return df.iloc[0].to_dict()


def step1_ingest_single(ticker: str, meta_info: dict, target_date, lookback_days: int = 7):
    """Step 1: ë‹¨ì¼ í‹°ì»¤ ë°ì´í„° ìˆ˜ì§‘"""
    print(f"\n{'='*70}")
    print(f"ğŸ“¥ Step 1: {ticker} ë°ì´í„° ìˆ˜ì§‘")
    print(f"{'='*70}")

    iso_code = meta_info["iso_code"]
    meta_id = meta_info["meta_id"]
    name = meta_info["name"]
    max_date = meta_info["max_date"]

    # ì‹œì‘ ë‚ ì§œ ê³„ì‚°
    if max_date:
        start_date = max_date - timedelta(days=lookback_days)
    else:
        start_date = datetime(2000, 1, 1).date()

    print(f"   ğŸ“Š meta_id: {meta_id}")
    print(f"   ğŸ“Š iso_code: {iso_code}")
    print(f"   ğŸ“Š max_date: {max_date}")
    print(f"   ğŸ“Š start_date: {start_date}")
    print(f"   ğŸ“Š target_date: {target_date}")

    # Yahoo ticker (KRì€ .KS ì¶”ê°€)
    yf_ticker = f"{ticker}.KS" if iso_code == "KR" else ticker

    # Yahoo Finance ë‹¤ìš´ë¡œë“œ
    print(f"\n   ğŸ”½ Yahoo Finance ë‹¤ìš´ë¡œë“œ ì¤‘... ({yf_ticker})")

    session = curl_requests.Session(impersonate="chrome")

    df = yf.download(
        tickers=yf_ticker,
        start=start_date,
        end=target_date + timedelta(days=1),
        progress=False,
        auto_adjust=False,
        session=session,
    )

    session.close()

    if df.empty:
        raise ValueError(f"No data downloaded for {yf_ticker}")

    print(f"   âœ… {len(df)} rows ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    print(f"   ğŸ“… ë²”ìœ„: {df.index.min().date()} ~ {df.index.max().date()}")

    # DataFrame ì •ì œ
    df = df[["Close", "Adj Close", "Volume"]].reset_index()
    df["ticker"] = ticker  # ì›ë³¸ ticker (KRì´ë©´ .KS ì œê±°)
    df.columns = ["trade_date", "close", "adj_close", "volume", "ticker"]

    df["trade_date"] = pd.to_datetime(df["trade_date"]).dt.date
    df["meta_id"] = meta_id
    df["name"] = name
    df["gross_return"] = None
    df["updated_at"] = datetime.now()

    # ì •ì œ
    df = df.dropna(subset=["close", "adj_close"])
    df["volume"] = df["volume"].fillna(0).astype("int64")

    print(f"   âœ… ì •ì œ í›„: {len(df)} rows")

    # Arrow Table ë³€í™˜
    arrow_schema = pa.schema(
        [
            pa.field("meta_id", pa.int32(), nullable=False),
            pa.field("trade_date", pa.date32(), nullable=False),
            pa.field("ticker", pa.string(), nullable=False),
            pa.field("name", pa.string(), nullable=True),
            pa.field("close", pa.float64(), nullable=True),
            pa.field("adj_close", pa.float64(), nullable=True),
            pa.field("gross_return", pa.float64(), nullable=True),
            pa.field("volume", pa.int64(), nullable=True),
            pa.field("updated_at", pa.timestamp("us"), nullable=False),
        ]
    )

    arrow_table = pa.Table.from_pandas(
        df[
            [
                "meta_id",
                "trade_date",
                "ticker",
                "name",
                "close",
                "adj_close",
                "gross_return",
                "volume",
                "updated_at",
            ]
        ],
        schema=arrow_schema,
    )

    # S3 ì €ì¥
    print(f"\n   ğŸ’¾ S3 ì €ì¥ ì¤‘...")

    staging_path = get_staging_path(iso_code, target_date.strftime("%Y-%m-%d"))
    # í…ŒìŠ¤íŠ¸ìš© ê²½ë¡œë¡œ ë³€ê²½
    staging_path = staging_path.replace("/staging/", "/staging-test/")

    s3_fs = pafs.S3FileSystem(region="ap-northeast-2")

    with s3_fs.open_output_stream(staging_path.replace("s3://", "")) as f:
        pq.write_table(arrow_table, f, compression="snappy")

    print(f"   âœ… {staging_path}")
    print(f"   ğŸ“Š {len(arrow_table)} rows, {arrow_table.nbytes / 1024:.2f} KB")

    return arrow_table, staging_path


def step2_transform_single(arrow_table: pa.Table, iso_code: str, target_date):
    """Step 2: ë‹¨ì¼ í‹°ì»¤ ë°ì´í„° ë³€í™˜"""
    print(f"\n{'='*70}")
    print(f"ğŸ”„ Step 2: ë°ì´í„° ë³€í™˜ (Iceberg ê¸°ì¤€)")
    print(f"{'='*70}")

    # meta_id ì¶”ì¶œ
    meta_ids = list(set(arrow_table.column("meta_id").to_pylist()))
    print(f"   ğŸ“Š meta_ids: {meta_ids}")

    # Iceberg ê¸°ì¤€ì  ì¡°íšŒ
    print(f"\n   ğŸ” Iceberg ê¸°ì¤€ì  ì¡°íšŒ ì¤‘...")
    last_iceberg_data = get_last_iceberg_data(iso_code, meta_ids)

    if last_iceberg_data:
        for mid, data in last_iceberg_data.items():
            print(f"      meta_id={mid}: {data['trade_date']}, adj_close={data['adj_close']:.4f}")
    else:
        print(f"      âš ï¸  Icebergì— ë°ì´í„° ì—†ìŒ (ì‹ ê·œ ì¢…ëª©)")

    # ì¬ê³„ì‚°
    print(f"\n   ğŸ”„ adj_close ì¬ê³„ì‚° ì¤‘...")
    transformed_table = recalculate_adj_close_and_returns(arrow_table, last_iceberg_data)

    if transformed_table is None:
        print(f"   âš ï¸  ì‹ ê·œ ë°ì´í„° ì—†ìŒ (ëª¨ë‘ ê¸°ì¡´ ë°ì´í„°)")
        return None, None

    print(f"   âœ… {len(transformed_table)} rows ë³€í™˜ ì™„ë£Œ")

    # ë°ì´í„° ê²€ì¦
    print(f"\n   ğŸ” ë°ì´í„° ê²€ì¦ ì¤‘...")
    cleaned_table, warnings = validate_data(transformed_table)

    if warnings:
        for warning in warnings:
            print(f"      âš ï¸  {warning}")

    print(f"   âœ… ìµœì¢…: {len(cleaned_table)} rows")

    # ìƒ˜í”Œ ì¶œë ¥
    print(f"\n   ğŸ“Š ë°ì´í„° ìƒ˜í”Œ:")
    sample_df = cleaned_table.to_pandas()[
        ["meta_id", "ticker", "trade_date", "close", "adj_close", "gross_return"]
    ]
    print(sample_df.head(10).to_string(index=False))
    print("   ...")
    print(sample_df.tail(5).to_string(index=False))

    # S3 ì €ì¥
    print(f"\n   ğŸ’¾ Transformed ì €ì¥ ì¤‘...")

    transformed_path = get_transformed_path(iso_code, target_date.strftime("%Y-%m-%d"))
    # í…ŒìŠ¤íŠ¸ìš© ê²½ë¡œë¡œ ë³€ê²½
    transformed_path = transformed_path.replace("/transformed/", "/transformed-test/")

    s3_fs = pafs.S3FileSystem(region="ap-northeast-2")

    with s3_fs.open_output_stream(transformed_path.replace("s3://", "")) as f:
        pq.write_table(cleaned_table, f, compression="snappy")

    print(f"   âœ… {transformed_path}")

    return cleaned_table, transformed_path


def step3_load_single(
    arrow_table: pa.Table, iso_code: str, target_date, database_url: str, dry_run: bool = True
):
    """Step 3: ë‹¨ì¼ í‹°ì»¤ ë°ì´í„° ì ì¬ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)"""
    print(f"\n{'='*70}")
    print(f"ğŸ’¾ Step 3: ë°ì´í„° ì ì¬")
    print(f"{'='*70}")

    if dry_run:
        print(f"\n   âš ï¸  DRY-RUN ëª¨ë“œ: ì‹¤ì œ ì ì¬í•˜ì§€ ì•ŠìŒ")
        print(f"\n   ğŸ“Š ì ì¬ ì˜ˆì • ë°ì´í„°:")
        print(f"      - iso_code: {iso_code}")
        print(f"      - rows: {len(arrow_table)}")

        # ë‚ ì§œ ë²”ìœ„
        trade_dates = arrow_table.column("trade_date").to_pylist()
        print(f"      - ë‚ ì§œ ë²”ìœ„: {min(trade_dates)} ~ {max(trade_dates)}")

        # ì¢…ëª©
        tickers = list(set(arrow_table.column("ticker").to_pylist()))
        print(f"      - ì¢…ëª©: {tickers}")

        print(f"\n   ğŸ’¡ ì‹¤ì œ ì ì¬í•˜ë ¤ë©´ --no-dry-run ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        return 0
    else:
        print(f"\n   âš ï¸  ì‹¤ì œ Icebergì— ì ì¬í•©ë‹ˆë‹¤!")
        loaded_count = load_to_iceberg(iso_code, arrow_table, target_date, database_url)
        return loaded_count


def main():
    parser = argparse.ArgumentParser(description="ë‹¨ì¼ í‹°ì»¤ ETL í…ŒìŠ¤íŠ¸")
    parser.add_argument("--ticker", type=str, required=True, help="í…ŒìŠ¤íŠ¸í•  í‹°ì»¤ (ì˜ˆ: SPY)")
    parser.add_argument("--date", type=str, default=None, help="ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD)")
    parser.add_argument(
        "--from",
        dest="from_step",
        type=str,
        default="step1",
        choices=["step1", "step2", "step3"],
        help="ì‹œì‘ Step",
    )
    parser.add_argument("--no-dry-run", action="store_true", help="ì‹¤ì œ Iceberg ì ì¬ (ì£¼ì˜!)")

    args = parser.parse_args()

    print("=" * 70)
    print(f"ğŸ§ª ë‹¨ì¼ í‹°ì»¤ ETL í…ŒìŠ¤íŠ¸: {args.ticker}")
    print("=" * 70)

    # í™˜ê²½ë³€ìˆ˜
    database_url = os.getenv("DATABASE_URL")
    if not database_url:
        raise ValueError("DATABASE_URL í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤!")

    # ë‚ ì§œ
    if args.date:
        target_date = datetime.strptime(args.date, "%Y-%m-%d").date()
    else:
        target_date = datetime.now().date() - timedelta(days=1)

    print(f"\nğŸ“… ë‚ ì§œ: {target_date}")
    print(f"â–¶ï¸  ì‹œì‘ì : {args.from_step}")
    print(f"ğŸ”’ Dry-run: {not args.no_dry_run}")

    try:
        # ë©”íƒ€ ì •ë³´ ì¡°íšŒ
        print(f"\nğŸ” {args.ticker} ë©”íƒ€ ì •ë³´ ì¡°íšŒ ì¤‘...")
        meta_info = get_ticker_meta(args.ticker, database_url)
        print(f"   âœ… meta_id: {meta_info['meta_id']}")
        print(f"   âœ… iso_code: {meta_info['iso_code']}")
        print(f"   âœ… name: {meta_info['name']}")
        print(f"   âœ… max_date: {meta_info['max_date']}")

        iso_code = meta_info["iso_code"]
        arrow_table = None

        # Step 1
        if args.from_step == "step1":
            arrow_table, staging_path = step1_ingest_single(args.ticker, meta_info, target_date)

        # Step 2
        if args.from_step in ["step1", "step2"]:
            if arrow_table is None:
                # stagingì—ì„œ ì½ê¸° (í…ŒìŠ¤íŠ¸ìš© ê²½ë¡œ)
                print(f"\nğŸ“‚ Staging íŒŒì¼ ì½ëŠ” ì¤‘...")
                import s3fs

                s3 = s3fs.S3FileSystem(anon=False)
                pattern = f"{S3_BUCKET}/staging-test/stocks/{iso_code}/{target_date.strftime('%Y-%m-%d')}_*.parquet"
                files = s3.glob(pattern)
                if not files:
                    raise FileNotFoundError(f"No staging-test files found: {pattern}")
                staging_path = f"s3://{sorted(files)[-1]}"
                arrow_table = pq.read_table(staging_path)
                print(f"   âœ… {staging_path}")

            arrow_table, transformed_path = step2_transform_single(
                arrow_table, iso_code, target_date
            )

            if arrow_table is None:
                print(f"\nâš ï¸  ë³€í™˜í•  ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

        # Step 3
        if args.from_step == "step3":
            # transformedì—ì„œ ì½ê¸° (í…ŒìŠ¤íŠ¸ìš© ê²½ë¡œ)
            print(f"\nğŸ“‚ Transformed íŒŒì¼ ì½ëŠ” ì¤‘...")
            import s3fs

            s3 = s3fs.S3FileSystem(anon=False)
            pattern = f"{S3_BUCKET}/transformed-test/stocks/{iso_code}/{target_date.strftime('%Y-%m-%d')}_*.parquet"
            files = s3.glob(pattern)
            if not files:
                raise FileNotFoundError(f"No transformed-test files found: {pattern}")
            transformed_path = f"s3://{sorted(files)[-1]}"
            arrow_table = pq.read_table(transformed_path)
            print(f"   âœ… {transformed_path}")

        if arrow_table is not None:
            loaded_count = step3_load_single(
                arrow_table, iso_code, target_date, database_url, dry_run=not args.no_dry_run
            )

        # ì™„ë£Œ
        print("\n" + "=" * 70)
        print(f"ğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {args.ticker}")
        print("=" * 70)

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main()
